import asyncio
import csv
import time
from datetime import datetime
from typing import List, Dict, Any
from pyppeteer import connect
from bs4 import BeautifulSoup
import re


class NhatotRealEstateCrawler:
    def __init__(self, browserless_url: str = "ws://localhost:3000", max_pages: int = 5):
        """
        Khá»Ÿi táº¡o crawler vá»›i browserless service
        
        Args:
            browserless_url: URL cá»§a browserless service (máº·c Ä‘á»‹nh localhost:3000)
            max_pages: Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl (máº·c Ä‘á»‹nh 5)
        """
        self.browserless_url = browserless_url
        self.max_pages = max_pages
        self.browser = None
        self.page = None
        self.scraped_data = []
        
    async def connect_browser(self):
        """Káº¿t ná»‘i Ä‘áº¿n browserless service báº±ng pyppeteer"""
        try:
            print("ğŸ”— Äang káº¿t ná»‘i Ä‘áº¿n browserless service...")
            self.browser = await connect(
                browserWSEndpoint=self.browserless_url,
                defaultViewport={'width': 1920, 'height': 1080}
            )
            print("âœ… ÄÃ£ káº¿t ná»‘i thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i káº¿t ná»‘i: {e}")
            return False
    
    async def create_page(self):
        """Táº¡o page má»›i"""
        try:
            self.page = await self.browser.newPage()
            
            # Set user agent Ä‘á»ƒ trÃ¡nh bá»‹ phÃ¡t hiá»‡n bot
            await self.page.setUserAgent(
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
            )
            
            # Set extra headers
            await self.page.setExtraHTTPHeaders({
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            })
            
            print("âœ… ÄÃ£ táº¡o page thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i táº¡o page: {e}")
            return False
    
    async def navigate_to_page(self, url: str, page_num: int = 1):
        """Navigate Ä‘áº¿n trang web vá»›i page number"""
        try:
            if page_num > 1:
                url = f"{url}?page={page_num}"
            
            print(f"ğŸŒ Äang truy cáº­p trang {page_num}: {url}")
            
            # Navigate vá»›i timeout
            await self.page.goto(url, {
                'waitUntil': 'networkidle2',
                'timeout': 30000
            })
            
            # Chá» thÃªm má»™t chÃºt Ä‘á»ƒ trang load hoÃ n toÃ n
            await asyncio.sleep(2)
            
            print(f"âœ… ÄÃ£ táº£i trang {page_num} thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i táº£i trang {page_num}: {e}")
            return False
    
    async def wait_for_content(self):
        """Chá» content load"""
        try:
            # Thá»­ nhiá»u selectors khÃ¡c nhau
            selectors = [
                'div[class*="AdItem"]',
                'a[class*="AdItem"]', 
                'div[class*="ad-item"]',
                'div[data-testid*="ad"]',
                'div[class*="Item"]',
                'a[href*="/bat-dong-san"]',
                '.list-item',
                '[class*="listing"]'
            ]
            
            for selector in selectors:
                try:
                    await self.page.waitForSelector(selector, {'timeout': 3000})
                    print(f"âœ… Content Ä‘Ã£ load vá»›i selector: {selector}")
                    return True
                except:
                    continue
            
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y content vá»›i selectors thÃ´ng thÆ°á»ng, tiáº¿p tá»¥c anyway...")
            return True  # Tiáº¿p tá»¥c crawl dÃ¹ khÃ´ng tÃ¬m tháº¥y specific selectors
        except Exception as e:
            print(f"âš ï¸ Lá»—i chá» content: {e}")
            return True  # Váº«n tiáº¿p tá»¥c
    
    async def get_page_content(self) -> str:
        """Láº¥y HTML content cá»§a trang"""
        try:
            content = await self.page.content()
            return content
        except Exception as e:
            print(f"âŒ Lá»—i láº¥y content: {e}")
            return ""
    
    async def check_page_exists(self, page_num: int) -> bool:
        """Kiá»ƒm tra trang cÃ³ tá»“n táº¡i khÃ´ng (cÃ³ data)"""
        try:
            # Kiá»ƒm tra page cÃ³ cÃ²n live khÃ´ng
            if self.page.isClosed():
                print(f"âš ï¸ Page Ä‘Ã£ bá»‹ Ä‘Ã³ng táº¡i trang {page_num}")
                return False
                
            # Äáº¿m sá»‘ lÆ°á»£ng items trÃªn trang vá»›i nhiá»u selectors
            items_count = await self.page.evaluate('''() => {
                const selectors = [
                    'div[class*="AdItem"]',
                    'a[class*="AdItem"]', 
                    'div[class*="ad-item"]',
                    'div[data-testid*="ad"]',
                    'div[class*="Item"]',
                    'a[href*="/bat-dong-san"]',
                    '.list-item',
                    '[class*="listing"]',
                    'div[class*="item"]',
                    'article',
                    '[class*="property"]'
                ];
                
                let maxCount = 0;
                for (const selector of selectors) {
                    try {
                        const items = document.querySelectorAll(selector);
                        maxCount = Math.max(maxCount, items.length);
                    } catch (e) {
                        continue;
                    }
                }
                return maxCount;
            }''')
            
            print(f"ğŸ“Š Trang {page_num}: TÃ¬m tháº¥y {items_count} items")
            return items_count > 0
        except Exception as e:
            print(f"âŒ Lá»—i kiá»ƒm tra trang {page_num}: {e}")
            return False
    
    def extract_property_data(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        Extract dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« HTML
        
        Args:
            html_content: HTML content cá»§a trang
            page_num: Sá»‘ trang hiá»‡n táº¡i
            
        Returns:
            List cÃ¡c dict chá»©a thÃ´ng tin báº¥t Ä‘á»™ng sáº£n
        """
        print(f"ğŸ” Äang extract dá»¯ liá»‡u tá»« trang {page_num}...")
        
        soup = BeautifulSoup(html_content, 'lxml')
        properties = []
        
        # TÃ¬m container chÃ­nh theo cáº¥u trÃºc: div.list-view>div>div.ListAds_ListAds__ANK2d>ul>div
        main_container = soup.select_one('div.list-view div div.ListAds_ListAds__ANK2d ul')
        
        if not main_container:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y container chÃ­nh vá»›i selector div.ListAds_ListAds__ANK2d")
        else:
            # TÃ¬m cÃ¡c div items trong container
            property_items = main_container.find_all('div', recursive=False)
            print(f"âœ… TÃ¬m tháº¥y container chÃ­nh, cÃ³ {len(property_items)} items")
        
        if not property_items:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y items nÃ o")
            return properties
            
        print(f"ğŸ  TÃ¬m tháº¥y {len(property_items)} báº¥t Ä‘á»™ng sáº£n trÃªn trang {page_num}")
        
        for idx, item in enumerate(property_items):
            try:
                property_data = self._extract_single_property(item, page_num, idx + 1)
                if property_data and property_data.get('title'):  # Chá»‰ thÃªm náº¿u cÃ³ title
                    properties.append(property_data)
            except Exception as e:
                print(f"âš ï¸ Lá»—i extract property {idx + 1} trang {page_num}: {e}")
                continue
        
        print(f"âœ… Extract thÃ nh cÃ´ng {len(properties)} báº¥t Ä‘á»™ng sáº£n tá»« trang {page_num}")
        return properties
    
    def _extract_single_property(self, item, page_num: int, item_idx: int) -> Dict[str, Any]:
        """Extract thÃ´ng tin tá»« má»™t item báº¥t Ä‘á»™ng sáº£n theo cáº¥u trÃºc HTML má»›i sá»­ dá»¥ng nth selectors"""
        property_data = {
            'title': '',
            'price': '',
            'area': '',
            'location': '',
            'description': '',
            'url': '',
            'image_url': '',
            'posted_date': '',
            'property_type': '',
            'bedrooms': '',
            'bathrooms': '',
            'page_number': page_num,
            'item_index': item_idx,
            'scraped_at': datetime.now().isoformat()
        }
        
        # TÃ¬m li element chá»©a itemListElement
        li_element = item.find('li', {'itemprop': 'itemListElement'})
        if not li_element:
            # Fallback: tÃ¬m trong chÃ­nh item
            li_element = item
        
        # TÃ¬m div chÃ­nh chá»©a content (thÆ°á»ng lÃ  div thá»© 2 trong structure)
        main_content_div = li_element.select_one('a[itemprop="item"] > div:nth-child(2)')
        if not main_content_div:
            # Fallback: tÃ¬m div cÃ³ chá»©a h3
            main_content_div = li_element.find('div', lambda value: value and li_element.find('h3'))
            if not main_content_div:
                main_content_div = li_element
        
        # Title - tÃ¬m h3 (thÆ°á»ng lÃ  element Ä‘áº§u tiÃªn trong content)
        title_elem = main_content_div.find('h3')
        if title_elem:
            property_data['title'] = title_elem.get_text(strip=True)
        
        # Property type - tÃ¬m span Ä‘áº§u tiÃªn sau h3
        spans_in_content = main_content_div.find_all('span', recursive=False)
        if len(spans_in_content) >= 1:
            type_text = spans_in_content[0].get_text(strip=True)
            property_data['property_type'] = type_text
            # Extract bedrooms/bathrooms náº¿u cÃ³ trong text
            if 'phÃ²ng ngá»§' in type_text.lower():
                bedrooms_match = re.search(r'(\d+)\s*phÃ²ng ngá»§', type_text, re.I)
                if bedrooms_match:
                    property_data['bedrooms'] = bedrooms_match.group(1)
            if 'phÃ²ng táº¯m' in type_text.lower() or 'wc' in type_text.lower():
                bathrooms_match = re.search(r'(\d+)\s*(?:phÃ²ng táº¯m|wc)', type_text, re.I)
                if bathrooms_match:
                    property_data['bathrooms'] = bathrooms_match.group(1)
        
        # Price vÃ  Area - tÃ¬m div chá»©a price info (thÆ°á»ng lÃ  div thá»© 2-3 trong content)
        price_div = main_content_div.find('div', lambda value: value and main_content_div.find('div').find_all('span'))
        
        if price_div:
            price_spans = price_div.find_all('span')
            for i, span in enumerate(price_spans):
                text = span.get_text(strip=True)
                style = span.get('style', '')
                
                # Price - thÆ°á»ng lÃ  span Ä‘áº§u tiÃªn hoáº·c span cÃ³ mÃ u Ä‘á»
                if not property_data['price'] and (
                    i == 0 or 
                    'rgb(229, 25, 59)' in style or 
                    any(keyword in text.lower() for keyword in ['tá»·', 'triá»‡u', 'Ä‘á»“ng', 'vnd'])
                ):
                    property_data['price'] = text
                
                # Area - tÃ¬m span chá»©a mÂ²
                if not property_data['area'] and ('mÂ²' in text or 'm2' in text):
                    property_data['area'] = text
        
        # Location vÃ  posted date - tÃ¬m span cuá»‘i cÃ¹ng trong content (thÆ°á»ng chá»©a location â€¢ date)
        location_span = None
        all_spans = main_content_div.find_all('span')
        # TÃ¬m span cuá»‘i cÃ¹ng cÃ³ chá»©a dáº¥u â€¢
        for span in reversed(all_spans):
            if 'â€¢' in span.get_text():
                location_span = span
                break
        
        if location_span:
            location_text = location_span.get_text(strip=True)
            # Split by â€¢ Ä‘á»ƒ tÃ¡ch location vÃ  date
            parts = [part.strip() for part in location_text.split('â€¢')]
            if len(parts) >= 1:
                property_data['location'] = parts[0]
            if len(parts) >= 2:
                property_data['posted_date'] = parts[1]
        
        # URL - tÃ¬m link chÃ­nh
        link_elem = li_element.find('a', {'itemprop': 'item'})
        if not link_elem:
            link_elem = li_element.find('a', href=True)
        
        if link_elem:
            href = link_elem.get('href', '')
            if href.startswith('/'):
                property_data['url'] = f"https://www.nhatot.com{href}"
            elif not href.startswith('http'):
                property_data['url'] = f"https://www.nhatot.com/{href}"
            else:
                property_data['url'] = href
        
        # Image URL - tÃ¬m img Ä‘áº§u tiÃªn
        img_elem = li_element.find('img', src=True)
        if img_elem:
            src = img_elem.get('src', '')
            alt = img_elem.get('alt', '')
            # Sá»­ dá»¥ng alt lÃ m description náº¿u cÃ³
            if alt and not property_data.get('description'):
                property_data['description'] = alt
                
            if src.startswith('//'):
                property_data['image_url'] = f"https:{src}"
            elif src.startswith('/'):
                property_data['image_url'] = f"https://www.nhatot.com{src}"
            else:
                property_data['image_url'] = src
        
        return property_data
    
    def save_to_csv(self, filename: str = None):
        """LÆ°u dá»¯ liá»‡u vÃ o file CSV"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"../real_estate_data_{timestamp}.csv"
        
        if not self.scraped_data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        print(f"ğŸ’¾ Äang lÆ°u {len(self.scraped_data)} báº¥t Ä‘á»™ng sáº£n vÃ o {filename}...")
        
        fieldnames = [
            'title', 'price', 'area', 'location', 'description', 
            'url', 'image_url', 'posted_date', 'property_type',
            'bedrooms', 'bathrooms', 'page_number', 'item_index', 'scraped_at'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.scraped_data)
        
        print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng vÃ o {filename}")
    
    async def crawl_nhatot_danang(self):
        """Crawl dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n ÄÃ  Náºµng tá»« nhatot.com theo pages"""
        base_url = "https://www.nhatot.com/mua-ban-bat-dong-san-da-nang"
        
        try:
            # Káº¿t ná»‘i browser
            if not await self.connect_browser():
                return False
            
            # Táº¡o page
            if not await self.create_page():
                return False
            
            # Crawl tá»«ng trang
            for page_num in range(1, self.max_pages + 1):
                print(f"\nğŸ”„ Äang crawl trang {page_num}/{self.max_pages}...")
                
                # Navigate Ä‘áº¿n trang
                if not await self.navigate_to_page(base_url, page_num):
                    print(f"âŒ KhÃ´ng thá»ƒ táº£i trang {page_num}, bá» qua...")
                    continue
                
                # Chá» content load
                await self.wait_for_content()
                
                # Kiá»ƒm tra trang cÃ³ data khÃ´ng
                if not await self.check_page_exists(page_num):
                    print(f"âš ï¸ Trang {page_num} khÃ´ng cÃ³ dá»¯ liá»‡u, dá»«ng crawl")
                    break
                
                # Láº¥y HTML content
                html_content = await self.get_page_content()
                
                if html_content:
                    # Extract dá»¯ liá»‡u
                    page_data = self.extract_property_data(html_content, page_num)
                    self.scraped_data.extend(page_data)
                    
                    print(f"âœ… Crawl trang {page_num}: +{len(page_data)} báº¥t Ä‘á»™ng sáº£n")
                else:
                    print(f"âŒ KhÃ´ng thá»ƒ láº¥y content tá»« trang {page_num}")
                
                # Delay giá»¯a cÃ¡c trang Ä‘á»ƒ trÃ¡nh bá»‹ block
                if page_num < self.max_pages:
                    print(f"â³ Chá» 2s trÆ°á»›c khi crawl trang tiáº¿p theo...")
                    await asyncio.sleep(2)
            
            # LÆ°u dá»¯ liá»‡u
            if self.scraped_data:
                self.save_to_csv()
                print(f"ğŸ‰ Crawl hoÃ n thÃ nh! Tá»•ng cá»™ng: {len(self.scraped_data)} báº¥t Ä‘á»™ng sáº£n tá»« {page_num} trang")
                return True
            else:
                print("âš ï¸ KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o")
                return False
                
        except Exception as e:
            print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {e}")
            return False
        finally:
            try:
                if self.page and not self.page.isClosed():
                    await self.page.close()
                    print("ğŸ” ÄÃ£ Ä‘Ã³ng page")
            except Exception as e:
                print(f"âš ï¸ Lá»—i Ä‘Ã³ng page: {e}")
            
            try:
                if self.browser:
                    await self.browser.disconnect()
                    print("ğŸ” ÄÃ£ Ä‘Ã³ng káº¿t ná»‘i browser")
            except Exception as e:
                print(f"âš ï¸ Lá»—i Ä‘Ã³ng browser: {e}")


async def async_main():
    """HÃ m async main Ä‘á»ƒ cháº¡y crawler"""
    print("ğŸš€ Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n ÄÃ  Náºµng tá»« nhatot.com")
    print("ğŸ“„ Crawl theo pages thay vÃ¬ scroll")
    print("=" * 60)
    
    # Khá»Ÿi táº¡o crawler vá»›i 5 trang
    crawler = NhatotRealEstateCrawler(max_pages=5)
    
    # Cháº¡y crawler
    success = await crawler.crawl_nhatot_danang()
    
    if success:
        print("âœ… Crawl hoÃ n thÃ nh thÃ nh cÃ´ng!")
    else:
        print("âŒ Crawl tháº¥t báº¡i!")
    
    print("=" * 60)
    return success


def main():
    """Entry point function for poetry scripts"""
    import sys
    try:
        success = asyncio.run(async_main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  Crawler bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
        sys.exit(130)
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng mong Ä‘á»£i: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # Cháº¡y crawler
    main()
