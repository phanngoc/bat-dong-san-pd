import asyncio
import csv
import time
from datetime import datetime
from typing import List, Dict, Any
from pyppeteer import connect
from bs4 import BeautifulSoup
import re


class NhatotRealEstateCrawler:
    def __init__(self, browserless_url: str = "ws://localhost:3000", max_pages: int = 5):
        """
        Khá»Ÿi táº¡o crawler vá»›i browserless service
        
        Args:
            browserless_url: URL cá»§a browserless service (máº·c Ä‘á»‹nh localhost:3000)
            max_pages: Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl (máº·c Ä‘á»‹nh 5)
        """
        self.browserless_url = browserless_url
        self.max_pages = max_pages
        self.browser = None
        self.page = None
        self.scraped_data = []
        
    async def connect_browser(self):
        """Káº¿t ná»‘i Ä‘áº¿n browserless service báº±ng pyppeteer"""
        try:
            print("ğŸ”— Äang káº¿t ná»‘i Ä‘áº¿n browserless service...")
            self.browser = await connect(
                browserWSEndpoint=self.browserless_url,
                defaultViewport={'width': 1920, 'height': 1080}
            )
            print("âœ… ÄÃ£ káº¿t ná»‘i thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i káº¿t ná»‘i: {e}")
            return False
    
    async def create_page(self):
        """Táº¡o page má»›i"""
        try:
            self.page = await self.browser.newPage()
            
            # Set user agent Ä‘á»ƒ trÃ¡nh bá»‹ phÃ¡t hiá»‡n bot
            await self.page.setUserAgent(
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
            )
            
            # Set extra headers
            await self.page.setExtraHTTPHeaders({
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            })
            
            print("âœ… ÄÃ£ táº¡o page thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i táº¡o page: {e}")
            return False
    
    async def navigate_to_page(self, url: str, page_num: int = 1):
        """Navigate Ä‘áº¿n trang web vá»›i page number"""
        try:
            if page_num > 1:
                url = f"{url}?page={page_num}"
            
            print(f"ğŸŒ Äang truy cáº­p trang {page_num}: {url}")
            
            # Navigate vá»›i timeout
            await self.page.goto(url, {
                'waitUntil': 'networkidle2',
                'timeout': 30000
            })
            
            # Chá» thÃªm má»™t chÃºt Ä‘á»ƒ trang load hoÃ n toÃ n
            await asyncio.sleep(2)
            
            print(f"âœ… ÄÃ£ táº£i trang {page_num} thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i táº£i trang {page_num}: {e}")
            return False
    
    async def get_page_content(self) -> str:
        """Láº¥y HTML content cá»§a trang"""
        try:
            content = await self.page.content()
            return content
        except Exception as e:
            print(f"âŒ Lá»—i láº¥y content: {e}")
            return ""
    
    def extract_property_data(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        Extract dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« HTML
        
        Args:
            html_content: HTML content cá»§a trang
            page_num: Sá»‘ trang hiá»‡n táº¡i
            
        Returns:
            List cÃ¡c dict chá»©a thÃ´ng tin báº¥t Ä‘á»™ng sáº£n
        """
        print(f"ğŸ” Äang extract dá»¯ liá»‡u tá»« trang {page_num}...")
        
        soup = BeautifulSoup(html_content, 'lxml')
        properties = []
        
        # TÃ¬m container chÃ­nh theo cáº¥u trÃºc: div.list-view>div>div.ListAds_ListAds__ANK2d>ul>div
        main_container = soup.select_one('div.list-view div div.ListAds_ListAds__ANK2d ul')
        
        if not main_container:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y container chÃ­nh vá»›i selector div.ListAds_ListAds__ANK2d")
        else:
            # TÃ¬m cÃ¡c div items trong container
            property_items = main_container.find_all('div', recursive=False)
            print(f"âœ… TÃ¬m tháº¥y container chÃ­nh, cÃ³ {len(property_items)} items")
        
        if not property_items:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y items nÃ o")
            return properties
            
        print(f"ğŸ  TÃ¬m tháº¥y {len(property_items)} báº¥t Ä‘á»™ng sáº£n trÃªn trang {page_num}")
        
        for idx, item in enumerate(property_items):
            try:
                property_data = self._extract_single_property(item, page_num, idx + 1)
                if property_data and property_data.get('title'):  # Chá»‰ thÃªm náº¿u cÃ³ title
                    properties.append(property_data)
            except Exception as e:
                print(f"âš ï¸ Lá»—i extract property {idx + 1} trang {page_num}: {e}")
                continue
        
        print(f"âœ… Extract thÃ nh cÃ´ng {len(properties)} báº¥t Ä‘á»™ng sáº£n tá»« trang {page_num}")
        return properties
    
    def _extract_single_property(self, item, page_num: int, item_idx: int) -> Dict[str, Any]:
        """Extract thÃ´ng tin tá»« má»™t item báº¥t Ä‘á»™ng sáº£n sá»­ dá»¥ng nth-child selectors"""
        property_data = {
            'title': '',
            'price': '',
            'price_unit': '',
            'area': '',
            'location': '',
            'description': '',
            'url': '',
            'image_url': '',
            'posted_date': '',
            'property_type': '',
            'bedrooms': '',
            'bathrooms': '',
            'page_number': page_num,
            'item_index': item_idx,
            'scraped_at': datetime.now().isoformat()
        }
        
        # TÃ¬m li element chá»©a itemListElement
        li_element = item.find('li', {'itemprop': 'itemListElement'})
        if not li_element:
            # Fallback: tÃ¬m trong chÃ­nh item
            li_element = item
        
        # TÃ¬m main content div - lÃ  div thá»© 2 trong a[itemprop="item"]
        main_content_div = li_element.select_one('a[itemprop="item"] > div:nth-child(2)')
        if not main_content_div:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y main content div cho item {item_idx}")
            return property_data
        
        # Title - tÃ¬m h3 (element thá»© 2 trong main content div)
        title_elem = main_content_div.select_one('h3')
        
        if title_elem and title_elem.name == 'h3':
            property_data['title'] = title_elem.get_text(strip=True)
        
        # Property type - tÃ¬m span thá»© 3 trong main content div
        property_type_elem = main_content_div.select_one(':nth-child(3)')
        if property_type_elem and property_type_elem.name == 'span':
            type_text = property_type_elem.get_text(strip=True)
            print("type_text", type_text)
            property_data['property_type'] = type_text
            # TÃ¡ch vÃ  xá»­ lÃ½ thÃ´ng tin tá»« type_text
            if type_text:
                parts = [part.strip() for part in type_text.split('â€¢')]
                for part in parts:
                    # Xá»­ lÃ½ sá»‘ phÃ²ng ngá»§
                    if 'PN' in part:
                        bedrooms = re.search(r'(\d+)\s*PN', part)
                        if bedrooms:
                            property_data['bedrooms'] = bedrooms.group(1)
                    
                    # Xá»­ lÃ½ hÆ°á»›ng nhÃ 
                    elif any(direction in part for direction in ['Nam', 'Báº¯c', 'ÄÃ´ng', 'TÃ¢y']):
                        property_data['direction'] = part.replace('HÆ°á»›ng ', '')
                    
                    # Xá»­ lÃ½ loáº¡i nhÃ 
                    else:
                        property_data['property_type'] = part
        # Price vÃ  Area - tÃ¬m div thá»© 4 trong main content div (chá»©a price info)
        price_div = main_content_div.select_one(':nth-child(4)')
        if price_div and price_div.name == 'div':
            # TÃ¬m táº¥t cáº£ span trong price div
            price_spans = price_div.find_all('span')
            print("price_spans", price_spans)
            property_data['price'] = price_spans[0].get_text(strip=True)
            property_data['price_unit'] = price_spans[1].get_text(strip=True)
            property_data['area'] = price_spans[2].get_text(strip=True)
        # Location vÃ  posted date - tÃ¬m span thá»© 5 trong main content div
        location_elem = main_content_div.find_all('span', recursive=False)[1]
        print("location_elem", location_elem)
        if location_elem and location_elem.name == 'span':
            location_text = location_elem.get_text(strip=True)
            print("location_text", location_text)
            # Split by â€¢ Ä‘á»ƒ tÃ¡ch location vÃ  date
            parts = [part.strip() for part in location_text.split('â€¢')]
            if len(parts) >= 1:
                property_data['location'] = parts[0]
            if len(parts) >= 2:
                property_data['posted_date'] = parts[1]
        
        # URL - tÃ¬m link chÃ­nh
        link_elem = li_element.find('a', {'itemprop': 'item'})
        if not link_elem:
            link_elem = li_element.find('a', href=True)
        
        if link_elem:
            href = link_elem.get('href', '')
            if href.startswith('/'):
                property_data['url'] = f"https://www.nhatot.com{href}"
            elif not href.startswith('http'):
                property_data['url'] = f"https://www.nhatot.com/{href}"
            else:
                property_data['url'] = href
        
        # Image URL - tÃ¬m img Ä‘áº§u tiÃªn trong li_element
        img_elem = li_element.find('img', src=True)
        if img_elem:
            src = img_elem.get('src', '')
            alt = img_elem.get('alt', '')
            
            # Sá»­ dá»¥ng alt lÃ m description náº¿u chÆ°a cÃ³ description
            if alt and not property_data.get('description'):
                property_data['description'] = alt
                
            if src.startswith('//'):
                property_data['image_url'] = f"https:{src}"
            elif src.startswith('/'):
                property_data['image_url'] = f"https://www.nhatot.com{src}"
            else:
                property_data['image_url'] = src
        
        return property_data
    
    def save_to_csv(self, filename: str = None):
        """LÆ°u dá»¯ liá»‡u vÃ o file CSV"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"../real_estate_data_{timestamp}.csv"
        
        if not self.scraped_data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        print(f"ğŸ’¾ Äang lÆ°u {len(self.scraped_data)} báº¥t Ä‘á»™ng sáº£n vÃ o {filename}...")
        
        fieldnames = [
            'title', 'price', 'area', 'location', 'description', 
            'url', 'image_url', 'posted_date', 'property_type',
            'bedrooms', 'bathrooms', 'page_number', 'item_index', 'scraped_at'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.scraped_data)
        
        print(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng vÃ o {filename}")
    
    async def crawl_nhatot_danang(self):
        """Crawl dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n ÄÃ  Náºµng tá»« nhatot.com theo pages"""
        base_url = "https://www.nhatot.com/mua-ban-bat-dong-san-da-nang"
        
        try:
            # Káº¿t ná»‘i browser
            if not await self.connect_browser():
                return False
            
            # Táº¡o page
            if not await self.create_page():
                return False
            
            # Crawl tá»«ng trang
            for page_num in range(1, self.max_pages + 1):
                print(f"\nğŸ”„ Äang crawl trang {page_num}/{self.max_pages}...")
                
                # Navigate Ä‘áº¿n trang
                if not await self.navigate_to_page(base_url, page_num):
                    print(f"âŒ KhÃ´ng thá»ƒ táº£i trang {page_num}, bá» qua...")
                    continue

                # Láº¥y HTML content
                html_content = await self.get_page_content()
                
                if html_content:
                    # Extract dá»¯ liá»‡u
                    page_data = self.extract_property_data(html_content, page_num)
                    self.scraped_data.extend(page_data)
                    
                    print(f"âœ… Crawl trang {page_num}: +{len(page_data)} báº¥t Ä‘á»™ng sáº£n")
                else:
                    print(f"âŒ KhÃ´ng thá»ƒ láº¥y content tá»« trang {page_num}")
                
                # Delay giá»¯a cÃ¡c trang Ä‘á»ƒ trÃ¡nh bá»‹ block
                if page_num < self.max_pages:
                    print(f"â³ Chá» 2s trÆ°á»›c khi crawl trang tiáº¿p theo...")
                    await asyncio.sleep(2)
            
            # LÆ°u dá»¯ liá»‡u
            if self.scraped_data:
                self.save_to_csv()
                print(f"ğŸ‰ Crawl hoÃ n thÃ nh! Tá»•ng cá»™ng: {len(self.scraped_data)} báº¥t Ä‘á»™ng sáº£n tá»« {page_num} trang")
                return True
            else:
                print("âš ï¸ KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o")
                return False
                
        except Exception as e:
            print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {e}")
            return False
        finally:
            try:
                if self.page and not self.page.isClosed():
                    await self.page.close()
                    print("ğŸ” ÄÃ£ Ä‘Ã³ng page")
            except Exception as e:
                print(f"âš ï¸ Lá»—i Ä‘Ã³ng page: {e}")
            
            try:
                if self.browser:
                    await self.browser.disconnect()
                    print("ğŸ” ÄÃ£ Ä‘Ã³ng káº¿t ná»‘i browser")
            except Exception as e:
                print(f"âš ï¸ Lá»—i Ä‘Ã³ng browser: {e}")


async def async_main():
    """HÃ m async main Ä‘á»ƒ cháº¡y crawler"""
    print("ğŸš€ Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n ÄÃ  Náºµng tá»« nhatot.com")
    print("ğŸ“„ Crawl theo pages thay vÃ¬ scroll")
    print("=" * 60)
    
    # Khá»Ÿi táº¡o crawler vá»›i 5 trang
    crawler = NhatotRealEstateCrawler(max_pages=5)
    
    # Cháº¡y crawler
    success = await crawler.crawl_nhatot_danang()
    
    if success:
        print("âœ… Crawl hoÃ n thÃ nh thÃ nh cÃ´ng!")
    else:
        print("âŒ Crawl tháº¥t báº¡i!")
    
    print("=" * 60)
    return success


def main():
    """Entry point function for poetry scripts"""
    import sys
    try:
        success = asyncio.run(async_main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  Crawler bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
        sys.exit(130)
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng mong Ä‘á»£i: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # Cháº¡y crawler
    main()
