import asyncio
import csv
import time
from datetime import datetime
from typing import List, Dict, Any
from pyppeteer import connect
from bs4 import BeautifulSoup
import re


class NhatotRealEstateCrawler:
    def __init__(self, browserless_url: str = "ws://localhost:3000", max_pages: int = 5):
        """
        Kh·ªüi t·∫°o crawler v·ªõi browserless service
        
        Args:
            browserless_url: URL c·ªßa browserless service (m·∫∑c ƒë·ªãnh localhost:3000)
            max_pages: S·ªë trang t·ªëi ƒëa ƒë·ªÉ crawl (m·∫∑c ƒë·ªãnh 5)
        """
        self.browserless_url = browserless_url
        self.max_pages = max_pages
        self.browser = None
        self.page = None
        self.scraped_data = []
        
    async def connect_browser(self):
        """K·∫øt n·ªëi ƒë·∫øn browserless service b·∫±ng pyppeteer"""
        try:
            print("üîó ƒêang k·∫øt n·ªëi ƒë·∫øn browserless service...")
            self.browser = await connect(
                browserWSEndpoint=self.browserless_url,
                defaultViewport={'width': 1920, 'height': 1080}
            )
            print("‚úÖ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng!")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi: {e}")
            return False
    
    async def create_page(self):
        """T·∫°o page m·ªõi"""
        try:
            self.page = await self.browser.newPage()
            
            # Set user agent ƒë·ªÉ tr√°nh b·ªã ph√°t hi·ªán bot
            await self.page.setUserAgent(
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
            )
            
            # Set extra headers
            await self.page.setExtraHTTPHeaders({
                'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            })
            
            print("‚úÖ ƒê√£ t·∫°o page th√†nh c√¥ng!")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o page: {e}")
            return False
    
    async def navigate_to_page(self, url: str, page_num: int = 1):
        """Navigate ƒë·∫øn trang web v·ªõi page number"""
        try:
            if page_num > 1:
                url = f"{url}?page={page_num}"
            
            print(f"üåê ƒêang truy c·∫≠p trang {page_num}: {url}")
            
            # Navigate v·ªõi timeout
            await self.page.goto(url, {
                'waitUntil': 'networkidle2',
                'timeout': 30000
            })
            
            # Ch·ªù th√™m m·ªôt ch√∫t ƒë·ªÉ trang load ho√†n to√†n
            await asyncio.sleep(2)
            
            print(f"‚úÖ ƒê√£ t·∫£i trang {page_num} th√†nh c√¥ng!")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i trang {page_num}: {e}")
            return False
    
    async def wait_for_content(self):
        """Ch·ªù content load"""
        try:
            # Th·ª≠ nhi·ªÅu selectors kh√°c nhau
            selectors = [
                'div[class*="AdItem"]',
                'a[class*="AdItem"]', 
                'div[class*="ad-item"]',
                'div[data-testid*="ad"]',
                'div[class*="Item"]',
                'a[href*="/bat-dong-san"]',
                '.list-item',
                '[class*="listing"]'
            ]
            
            for selector in selectors:
                try:
                    await self.page.waitForSelector(selector, {'timeout': 3000})
                    print(f"‚úÖ Content ƒë√£ load v·ªõi selector: {selector}")
                    return True
                except:
                    continue
            
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y content v·ªõi selectors th√¥ng th∆∞·ªùng, ti·∫øp t·ª•c anyway...")
            return True  # Ti·∫øp t·ª•c crawl d√π kh√¥ng t√¨m th·∫•y specific selectors
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói ch·ªù content: {e}")
            return True  # V·∫´n ti·∫øp t·ª•c
    
    async def get_page_content(self) -> str:
        """L·∫•y HTML content c·ªßa trang"""
        try:
            content = await self.page.content()
            return content
        except Exception as e:
            print(f"‚ùå L·ªói l·∫•y content: {e}")
            return ""
    
    async def check_page_exists(self, page_num: int) -> bool:
        """Ki·ªÉm tra trang c√≥ t·ªìn t·∫°i kh√¥ng (c√≥ data)"""
        try:
            # Ki·ªÉm tra page c√≥ c√≤n live kh√¥ng
            if self.page.isClosed():
                print(f"‚ö†Ô∏è Page ƒë√£ b·ªã ƒë√≥ng t·∫°i trang {page_num}")
                return False
                
            # ƒê·∫øm s·ªë l∆∞·ª£ng items tr√™n trang v·ªõi nhi·ªÅu selectors
            items_count = await self.page.evaluate('''() => {
                const selectors = [
                    'div[class*="AdItem"]',
                    'a[class*="AdItem"]', 
                    'div[class*="ad-item"]',
                    'div[data-testid*="ad"]',
                    'div[class*="Item"]',
                    'a[href*="/bat-dong-san"]',
                    '.list-item',
                    '[class*="listing"]',
                    'div[class*="item"]',
                    'article',
                    '[class*="property"]'
                ];
                
                let maxCount = 0;
                for (const selector of selectors) {
                    try {
                        const items = document.querySelectorAll(selector);
                        maxCount = Math.max(maxCount, items.length);
                    } catch (e) {
                        continue;
                    }
                }
                return maxCount;
            }''')
            
            print(f"üìä Trang {page_num}: T√¨m th·∫•y {items_count} items")
            return items_count > 0
        except Exception as e:
            print(f"‚ùå L·ªói ki·ªÉm tra trang {page_num}: {e}")
            return False
    
    def extract_property_data(self, html_content: str, page_num: int) -> List[Dict[str, Any]]:
        """
        Extract d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n t·ª´ HTML
        
        Args:
            html_content: HTML content c·ªßa trang
            page_num: S·ªë trang hi·ªán t·∫°i
            
        Returns:
            List c√°c dict ch·ª©a th√¥ng tin b·∫•t ƒë·ªông s·∫£n
        """
        print(f"üîç ƒêang extract d·ªØ li·ªáu t·ª´ trang {page_num}...")
        
        soup = BeautifulSoup(html_content, 'lxml')
        properties = []
        
        # T√¨m c√°c item b·∫•t ƒë·ªông s·∫£n v·ªõi nhi·ªÅu selector kh√°c nhau
        selectors = [
            'div[class*="AdItem_adItem"]',
            'div[class*="AdItem"]', 
            'a[class*="AdItem"]',
            'div[class*="ad-item"]',
            'div[data-testid*="ad"]',
            'div[class*="Item"]',
            'a[href*="/bat-dong-san"]',
            '.list-item',
            '[class*="listing"]',
            'div[class*="item"]',
            'article',
            '[class*="property"]',
            'div[class*="card"]',
            'li[class*="item"]'
        ]
        
        property_items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                property_items = items
                print(f"‚úÖ S·ª≠ d·ª•ng selector: {selector}")
                break
        
        if not property_items:
            # Fallback 1: t√¨m t·∫•t c·∫£ links c√≥ href ch·ª©a bat-dong-san
            property_items = soup.find_all('a', href=re.compile(r'.*bat-dong-san.*', re.I))
            if property_items:
                print("‚úÖ S·ª≠ d·ª•ng fallback: links ch·ª©a 'bat-dong-san'")
            
        if not property_items:
            # Fallback 2: t√¨m t·∫•t c·∫£ divs c√≥ ch·ª©a text gi√° ti·ªÅn
            property_items = soup.find_all('div', string=re.compile(r'.*t·ª∑.*|.*tri·ªáu.*|.*million.*', re.I))
            if property_items:
                print("‚úÖ S·ª≠ d·ª•ng fallback: divs ch·ª©a gi√° ti·ªÅn")
                
        if not property_items:
            # Fallback 3: t√¨m t·∫•t c·∫£ articles
            property_items = soup.find_all('article')
            if property_items:
                print("‚úÖ S·ª≠ d·ª•ng fallback: article tags")
                
        if not property_items:
            # Fallback 4: t√¨m t·∫•t c·∫£ divs c√≥ class ch·ª©a 'item' ho·∫∑c 'card'
            property_items = soup.find_all('div', class_=re.compile(r'.*item.*|.*card.*', re.I))
            if property_items:
                print("‚úÖ S·ª≠ d·ª•ng fallback: divs v·ªõi class item/card")
            
        print(f"üè† T√¨m th·∫•y {len(property_items)} b·∫•t ƒë·ªông s·∫£n tr√™n trang {page_num}")
        
        for idx, item in enumerate(property_items):
            try:
                property_data = self._extract_single_property(item, page_num, idx + 1)
                if property_data and property_data.get('title'):  # Ch·ªâ th√™m n·∫øu c√≥ title
                    properties.append(property_data)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói extract property {idx + 1} trang {page_num}: {e}")
                continue
        
        print(f"‚úÖ Extract th√†nh c√¥ng {len(properties)} b·∫•t ƒë·ªông s·∫£n t·ª´ trang {page_num}")
        return properties
    
    def _extract_single_property(self, item, page_num: int, item_idx: int) -> Dict[str, Any]:
        """Extract th√¥ng tin t·ª´ m·ªôt item b·∫•t ƒë·ªông s·∫£n"""
        property_data = {
            'title': '',
            'price': '',
            'area': '',
            'location': '',
            'description': '',
            'url': '',
            'image_url': '',
            'posted_date': '',
            'property_type': '',
            'bedrooms': '',
            'bathrooms': '',
            'page_number': page_num,
            'item_index': item_idx,
            'scraped_at': datetime.now().isoformat()
        }
        
        # T√¨m parent container n·∫øu c·∫ßn
        container = item
        for _ in range(3):  # T√¨m trong 3 level parent
            parent = container.parent if container.parent else container
            if parent.find('a', href=True):
                container = parent
                break
            container = parent
        
        # Title - th·ª≠ nhi·ªÅu selector
        title_selectors = [
            'h3', 'h2', 'h4', 'h5',
            '[class*="title"]', '[class*="heading"]', 
            '[class*="subject"]', '[class*="name"]'
        ]
        
        for selector in title_selectors:
            title_elem = container.select_one(selector)
            if title_elem and title_elem.get_text(strip=True):
                property_data['title'] = title_elem.get_text(strip=True)
                break
        
        # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c title, th·ª≠ t√¨m text trong links
        if not property_data['title']:
            link_elem = container.find('a')
            if link_elem:
                property_data['title'] = link_elem.get_text(strip=True)
        
        # Price - t√¨m text ch·ª©a ti·ªÅn
        price_patterns = [r'.*t·ª∑.*', r'.*tri·ªáu.*', r'.*ƒë·ªìng.*', r'.*VND.*', r'.*million.*']
        for pattern in price_patterns:
            price_elem = container.find(text=re.compile(pattern, re.I))
            if price_elem:
                property_data['price'] = price_elem.strip()
                break
        
        # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c, th·ª≠ t√¨m trong span/div c√≥ class ch·ª©a price
        if not property_data['price']:
            price_elem = container.find(['span', 'div'], class_=re.compile(r'.*price.*|.*gia.*', re.I))
            if price_elem:
                property_data['price'] = price_elem.get_text(strip=True)
        
        # Area - t√¨m text ch·ª©a m¬≤
        area_elem = container.find(text=re.compile(r'.*m¬≤.*|.*m2.*|.*di·ªán t√≠ch.*', re.I))
        if area_elem:
            property_data['area'] = area_elem.strip()
        
        # Location - t√¨m ƒë·ªãa ch·ªâ
        location_selectors = [
            '[class*="location"]', '[class*="address"]', 
            '[class*="dia-chi"]', '[class*="diadiem"]'
        ]
        for selector in location_selectors:
            location_elem = container.select_one(selector)
            if location_elem:
                property_data['location'] = location_elem.get_text(strip=True)
                break
        
        # URL
        link_elem = container.find('a', href=True)
        if link_elem:
            href = link_elem['href']
            if href.startswith('/'):
                property_data['url'] = f"https://www.nhatot.com{href}"
            elif not href.startswith('http'):
                property_data['url'] = f"https://www.nhatot.com/{href}"
            else:
                property_data['url'] = href
        
        # Image URL
        img_elem = container.find('img', src=True)
        if img_elem:
            src = img_elem['src']
            if src.startswith('//'):
                property_data['image_url'] = f"https:{src}"
            elif src.startswith('/'):
                property_data['image_url'] = f"https://www.nhatot.com{src}"
            else:
                property_data['image_url'] = src
        
        return property_data
    
    def save_to_csv(self, filename: str = None):
        """L∆∞u d·ªØ li·ªáu v√†o file CSV"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"../real_estate_data_{timestamp}.csv"
        
        if not self.scraped_data:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        print(f"üíæ ƒêang l∆∞u {len(self.scraped_data)} b·∫•t ƒë·ªông s·∫£n v√†o {filename}...")
        
        fieldnames = [
            'title', 'price', 'area', 'location', 'description', 
            'url', 'image_url', 'posted_date', 'property_type',
            'bedrooms', 'bathrooms', 'page_number', 'item_index', 'scraped_at'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.scraped_data)
        
        print(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng v√†o {filename}")
    
    async def crawl_nhatot_danang(self):
        """Crawl d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n ƒê√† N·∫µng t·ª´ nhatot.com theo pages"""
        base_url = "https://www.nhatot.com/mua-ban-bat-dong-san-da-nang"
        
        try:
            # K·∫øt n·ªëi browser
            if not await self.connect_browser():
                return False
            
            # T·∫°o page
            if not await self.create_page():
                return False
            
            # Crawl t·ª´ng trang
            for page_num in range(1, self.max_pages + 1):
                print(f"\nüîÑ ƒêang crawl trang {page_num}/{self.max_pages}...")
                
                # Navigate ƒë·∫øn trang
                if not await self.navigate_to_page(base_url, page_num):
                    print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i trang {page_num}, b·ªè qua...")
                    continue
                
                # Ch·ªù content load
                await self.wait_for_content()
                
                # Ki·ªÉm tra trang c√≥ data kh√¥ng
                if not await self.check_page_exists(page_num):
                    print(f"‚ö†Ô∏è Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu, d·ª´ng crawl")
                    break
                
                # L·∫•y HTML content
                html_content = await self.get_page_content()
                
                if html_content:
                    # Extract d·ªØ li·ªáu
                    page_data = self.extract_property_data(html_content, page_num)
                    self.scraped_data.extend(page_data)
                    
                    print(f"‚úÖ Crawl trang {page_num}: +{len(page_data)} b·∫•t ƒë·ªông s·∫£n")
                else:
                    print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y content t·ª´ trang {page_num}")
                
                # Delay gi·ªØa c√°c trang ƒë·ªÉ tr√°nh b·ªã block
                if page_num < self.max_pages:
                    print(f"‚è≥ Ch·ªù 2s tr∆∞·ªõc khi crawl trang ti·∫øp theo...")
                    await asyncio.sleep(2)
            
            # L∆∞u d·ªØ li·ªáu
            if self.scraped_data:
                self.save_to_csv()
                print(f"üéâ Crawl ho√†n th√†nh! T·ªïng c·ªông: {len(self.scraped_data)} b·∫•t ƒë·ªông s·∫£n t·ª´ {page_num} trang")
                return True
            else:
                print("‚ö†Ô∏è Kh√¥ng crawl ƒë∆∞·ª£c d·ªØ li·ªáu n√†o")
                return False
                
        except Exception as e:
            print(f"‚ùå L·ªói trong qu√° tr√¨nh crawl: {e}")
            return False
        finally:
            try:
                if self.page and not self.page.isClosed():
                    await self.page.close()
                    print("üîê ƒê√£ ƒë√≥ng page")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói ƒë√≥ng page: {e}")
            
            try:
                if self.browser:
                    await self.browser.disconnect()
                    print("üîê ƒê√£ ƒë√≥ng k·∫øt n·ªëi browser")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói ƒë√≥ng browser: {e}")


async def async_main():
    """H√†m async main ƒë·ªÉ ch·∫°y crawler"""
    print("üöÄ B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n ƒê√† N·∫µng t·ª´ nhatot.com")
    print("üìÑ Crawl theo pages thay v√¨ scroll")
    print("=" * 60)
    
    # Kh·ªüi t·∫°o crawler v·ªõi 5 trang
    crawler = NhatotRealEstateCrawler(max_pages=5)
    
    # Ch·∫°y crawler
    success = await crawler.crawl_nhatot_danang()
    
    if success:
        print("‚úÖ Crawl ho√†n th√†nh th√†nh c√¥ng!")
    else:
        print("‚ùå Crawl th·∫•t b·∫°i!")
    
    print("=" * 60)
    return success


def main():
    """Entry point function for poetry scripts"""
    import sys
    try:
        success = asyncio.run(async_main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Crawler b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        sys.exit(130)
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # Ch·∫°y crawler
    main()
